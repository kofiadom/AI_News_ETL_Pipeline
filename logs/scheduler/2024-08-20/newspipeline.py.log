[2024-08-20T23:10:43.958+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:10:43.959+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:10:43.976+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:10:43.975+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:11:11.559+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:11:11.752+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:11:11.839+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:11:12.137+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:12.137+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:etl_data_pipeline
[2024-08-20T23:11:12.157+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:12.157+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:etl_data_pipeline
[2024-08-20T23:11:12.163+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:12.162+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:etl_data_pipeline
[2024-08-20T23:11:12.163+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:12.163+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:11:12.173+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:12.173+0000] {dag.py:3118} INFO - Creating ORM DAG for etl_data_pipeline
[2024-08-20T23:11:12.183+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:12.183+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-19 00:00:00+00:00, run_after=2024-08-20 00:00:00+00:00
[2024-08-20T23:11:12.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 28.246 seconds
[2024-08-20T23:11:42.447+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:11:42.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:11:42.500+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:42.500+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:11:47.607+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:11:47.769+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:11:47.788+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:11:47.823+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:47.822+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:11:47.842+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:11:47.842+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-19 00:00:00+00:00, run_after=2024-08-20 00:00:00+00:00
[2024-08-20T23:11:47.873+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.441 seconds
[2024-08-20T23:12:18.100+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:12:18.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:12:18.122+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:12:18.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:12:21.846+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:12:32.275+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:12:32.139+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    load_data_to_postgres(df, table_name, conn_string)
  File "/opt/airflow/dags/newspipeline.py", line 63, in load_data_to_postgres
    conn = psycopg2.connect(conn_string)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution
[2024-08-20T23:12:32.280+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:12:42.448+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 843, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 622, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(errors.ImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-08-20T23:13:15.521+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:13:15.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:13:15.652+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:13:15.534+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:13:19.107+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:13:19.311+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:13:19.332+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:13:19.385+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:13:19.384+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:13:19.407+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:13:19.407+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-19 00:00:00+00:00, run_after=2024-08-20 00:00:00+00:00
[2024-08-20T23:13:19.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.923 seconds
[2024-08-20T23:13:49.812+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:13:49.822+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:13:49.825+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:13:49.824+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:13:55.308+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:13:55.805+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:13:56.367+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:13:56.613+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:13:56.607+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:14:01.978+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:14:01.973+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:14:04.981+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 15.181 seconds
[2024-08-20T23:15:01.147+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:15:01.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:15:01.220+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:15:01.173+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:16:06.970+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:16:06.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:16:06.972+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:16:06.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:16:41.632+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:16:41.617+0000] {timeout.py:68} ERROR - Process timed out, PID: 131
[2024-08-20T23:17:14.376+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:17:14.377+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:17:14.378+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:17:14.378+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:17:17.875+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:17:18.222+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:17:18.252+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:17:18.673+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:17:18.663+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:17:18.715+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:17:18.713+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:17:18.777+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.407 seconds
[2024-08-20T23:17:48.957+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:17:48.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:17:48.959+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:17:48.959+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:17:52.071+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:17:52.436+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:17:52.454+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:17:52.518+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:17:52.517+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:17:52.540+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:17:52.540+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:17:52.566+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.615 seconds
[2024-08-20T23:18:22.807+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:18:22.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:18:22.810+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:18:22.809+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:18:25.754+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:18:25.978+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:18:25.994+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:18:26.046+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:18:26.046+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:18:26.066+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:18:26.066+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:18:26.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.298 seconds
[2024-08-20T23:18:56.238+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:18:56.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:18:56.240+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:18:56.240+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:18:59.172+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:18:59.288+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:18:59.298+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:18:59.335+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:18:59.334+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:18:59.353+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:18:59.353+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:18:59.374+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.143 seconds
[2024-08-20T23:19:29.942+0000] {processor.py:161} INFO - Started process (PID=177) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:19:29.945+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:19:29.947+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:19:29.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:20:00.081+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:20:00.070+0000] {timeout.py:68} ERROR - Process timed out, PID: 177
[2024-08-20T23:20:51.774+0000] {processor.py:161} INFO - Started process (PID=185) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:20:51.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:20:51.822+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:20:51.813+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:22:21.387+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:22:21.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:22:21.479+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:22:21.445+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:24:28.399+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:24:28.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:24:28.653+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:24:28.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:25:31.647+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:25:31.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:25:31.649+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:25:31.649+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:25:35.858+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:25:36.018+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:25:36.048+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:25:36.211+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:25:36.211+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:25:36.233+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:25:36.233+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:25:36.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.619 seconds
[2024-08-20T23:26:16.358+0000] {processor.py:161} INFO - Started process (PID=201) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:26:16.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:26:16.596+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:26:16.501+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:26:57.765+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:26:46.816+0000] {timeout.py:68} ERROR - Process timed out, PID: 201
[2024-08-20T23:27:42.263+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:27:42.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:27:42.267+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:27:42.266+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:27:45.643+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:27:45.735+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:27:45.744+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:27:45.829+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:27:45.828+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:27:45.850+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:27:45.850+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:27:45.867+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.609 seconds
[2024-08-20T23:28:16.045+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:28:16.047+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:28:16.049+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:28:16.049+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:28:19.205+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:28:19.457+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:28:19.474+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:28:19.509+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:28:19.508+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:28:19.531+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:28:19.531+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:28:19.551+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.513 seconds
[2024-08-20T23:28:50.010+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:28:50.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:28:50.023+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:28:50.021+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:29:01.838+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:29:02.036+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:29:02.282+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:29:02.404+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:29:02.402+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:29:02.443+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:29:02.443+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:29:02.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 12.531 seconds
[2024-08-20T23:29:32.640+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:29:32.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:29:32.646+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:29:32.645+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:29:53.389+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:29:53.758+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:29:53.799+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:29:55.189+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:29:55.182+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:29:55.252+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:29:55.249+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:29:55.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 22.669 seconds
[2024-08-20T23:30:25.721+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:30:25.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:30:26.212+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:30:25.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:30:39.753+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:30:40.433+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:30:40.462+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:30:41.804+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:30:41.790+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:30:41.908+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:30:41.901+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:30:41.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 16.299 seconds
[2024-08-20T23:31:12.222+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:31:12.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:31:12.229+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:31:12.228+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:31:14.738+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:31:14.828+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:31:14.844+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:31:14.880+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:31:14.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:31:14.899+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:31:14.899+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:31:14.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.706 seconds
[2024-08-20T23:31:45.962+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:31:46.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:31:52.838+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:31:49.935+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:06.762+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:06.763+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:33:06.764+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:33:06.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:10.137+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:33:10.224+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:33:10.238+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:10.407+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:33:10.407+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:33:10.429+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:33:10.428+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:33:10.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.688 seconds
[2024-08-20T23:33:40.847+0000] {processor.py:161} INFO - Started process (PID=132) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:40.848+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:33:40.849+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:33:40.848+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:43.391+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:33:43.477+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:33:43.495+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:33:43.529+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:33:43.529+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:33:43.550+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:33:43.550+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:33:43.569+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.734 seconds
[2024-08-20T23:34:13.761+0000] {processor.py:161} INFO - Started process (PID=140) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:34:13.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:34:13.763+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:34:13.763+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:34:16.541+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:34:16.623+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:34:16.639+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:34:16.672+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:34:16.672+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:34:16.693+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:34:16.693+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:34:16.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.957 seconds
[2024-08-20T23:34:46.876+0000] {processor.py:161} INFO - Started process (PID=148) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:34:46.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:34:46.886+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:34:46.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:34:58.531+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:34:58.600+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:34:58.615+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:34:58.654+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:34:58.653+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:34:58.674+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:34:58.674+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:34:58.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 11.851 seconds
[2024-08-20T23:35:28.813+0000] {processor.py:161} INFO - Started process (PID=162) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:35:28.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:35:28.814+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:35:28.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:36:03.229+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:36:03.213+0000] {timeout.py:68} ERROR - Process timed out, PID: 162
[2024-08-20T23:36:34.409+0000] {processor.py:161} INFO - Started process (PID=170) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:36:34.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:36:34.416+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:36:34.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:36:54.312+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:36:55.108+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:36:55.468+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:36:55.945+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:36:55.936+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:36:56.002+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:36:56.002+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:36:56.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 21.663 seconds
[2024-08-20T23:37:26.625+0000] {processor.py:161} INFO - Started process (PID=185) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:37:26.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:37:26.628+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:37:26.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:37:29.865+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:37:30.031+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:37:30.044+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:37:30.073+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:37:30.073+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:37:30.094+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:37:30.094+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:37:30.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.496 seconds
[2024-08-20T23:38:00.289+0000] {processor.py:161} INFO - Started process (PID=192) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:38:00.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:38:00.291+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:38:00.291+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:38:03.488+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:38:03.789+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:38:03.798+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:38:06.251+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:38:06.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:38:06.359+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:38:06.358+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:38:06.462+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 6.177 seconds
[2024-08-20T23:38:37.424+0000] {processor.py:161} INFO - Started process (PID=200) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:38:37.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:38:41.019+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:38:41.018+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:39:15.673+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:39:15.637+0000] {timeout.py:68} ERROR - Process timed out, PID: 200
[2024-08-20T23:39:49.258+0000] {processor.py:161} INFO - Started process (PID=208) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:39:49.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:39:49.699+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:39:49.699+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:40:22.191+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:40:21.905+0000] {timeout.py:68} ERROR - Process timed out, PID: 208
[2024-08-20T23:40:55.711+0000] {processor.py:161} INFO - Started process (PID=222) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:40:55.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:40:56.858+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:40:56.857+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:41:08.916+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:41:09.112+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:41:09.137+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:41:09.353+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:41:09.352+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:41:09.372+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:41:09.372+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:41:09.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 14.401 seconds
[2024-08-20T23:45:17.719+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:45:17.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:45:17.731+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:45:17.731+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:45:34.921+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:45:35.008+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:45:35.023+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:45:35.264+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:45:35.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:45:35.286+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:45:35.285+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:45:35.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 17.612 seconds
[2024-08-20T23:46:05.511+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:46:05.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:46:05.518+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:46:05.515+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:46:08.452+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:46:08.569+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:46:08.579+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:46:08.606+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:46:08.606+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:46:08.625+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:46:08.625+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:46:08.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.142 seconds
[2024-08-20T23:46:38.752+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:46:38.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:46:38.755+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:46:38.754+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:46:41.534+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:46:41.626+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:46:41.636+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:46:41.666+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:46:41.665+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:46:41.685+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:46:41.685+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:46:41.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.965 seconds
[2024-08-20T23:47:11.867+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:47:11.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:47:11.870+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:47:11.870+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:47:14.380+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:47:14.505+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:47:14.513+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:47:14.541+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:47:14.540+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:47:14.563+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:47:14.563+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:47:14.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.731 seconds
[2024-08-20T23:47:44.737+0000] {processor.py:161} INFO - Started process (PID=91) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:47:44.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:47:44.740+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:47:44.739+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:47:47.322+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:47:47.399+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:47:47.411+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:47:47.608+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:47:47.608+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:47:47.625+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:47:47.625+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:47:47.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.911 seconds
[2024-08-20T23:48:17.781+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:48:17.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:48:17.784+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:48:17.783+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:48:20.270+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:48:20.381+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:48:20.393+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:48:20.426+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:48:20.425+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:48:20.444+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:48:20.444+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:48:20.459+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.688 seconds
[2024-08-20T23:48:50.595+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:48:50.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:48:50.597+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:48:50.597+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:48:53.190+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:48:53.271+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:48:53.280+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:48:53.310+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:48:53.310+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:48:53.329+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:48:53.329+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:48:53.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.757 seconds
[2024-08-20T23:49:23.458+0000] {processor.py:161} INFO - Started process (PID=121) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:49:23.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:49:23.461+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:49:23.460+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:49:26.068+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:49:26.147+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:49:26.156+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:49:26.181+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:49:26.180+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:49:26.199+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:49:26.199+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:49:26.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.761 seconds
[2024-08-20T23:49:56.358+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:49:56.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:49:56.360+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:49:56.360+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:49:59.150+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:49:59.239+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:49:59.250+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:49:59.283+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:49:59.283+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:49:59.304+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:49:59.303+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:49:59.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.969 seconds
[2024-08-20T23:50:29.476+0000] {processor.py:161} INFO - Started process (PID=137) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:50:29.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:50:29.480+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:50:29.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:50:41.879+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:50:42.070+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:50:42.089+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:50:42.198+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:50:42.197+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:50:42.226+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:50:42.226+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:50:42.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 12.784 seconds
[2024-08-20T23:51:12.385+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:51:12.386+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:51:12.388+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:51:12.387+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:51:15.202+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:51:16.473+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:51:16.521+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:51:33.059+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:51:33.057+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:51:33.084+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:51:33.083+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:51:33.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 20.750 seconds
[2024-08-20T23:52:03.829+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:52:03.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:52:03.853+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:52:03.852+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:52:06.534+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:52:06.805+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:52:06.835+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:52:07.604+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:52:07.585+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:52:07.790+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:52:07.786+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:52:08.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.716 seconds
[2024-08-20T23:52:38.714+0000] {processor.py:161} INFO - Started process (PID=167) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:52:38.715+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:52:38.717+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:52:38.717+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:52:41.380+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:52:41.535+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:52:41.556+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:52:41.638+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:52:41.637+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:52:41.661+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:52:41.661+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:52:41.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.975 seconds
[2024-08-20T23:53:11.836+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:53:12.181+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:53:12.183+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:53:12.182+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:53:15.733+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:53:16.011+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:53:16.032+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:53:16.232+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:53:16.228+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:53:16.251+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:53:16.251+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:53:16.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.440 seconds
[2024-08-20T23:53:46.558+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:53:46.603+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:53:46.607+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:53:46.605+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:53:54.455+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:53:54.782+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:53:54.821+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:53:55.251+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:53:55.239+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:53:55.302+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:53:55.298+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:53:55.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 8.838 seconds
[2024-08-20T23:54:26.068+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:54:26.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:54:26.072+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:54:26.071+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:54:30.759+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:54:31.032+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:54:31.646+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:54:31.904+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:54:31.903+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:54:31.932+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:54:31.932+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:54:31.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.916 seconds
[2024-08-20T23:55:11.454+0000] {processor.py:161} INFO - Started process (PID=192) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:55:11.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:55:11.458+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:55:11.457+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:55:33.882+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:55:34.187+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:55:34.219+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:55:34.500+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:55:34.486+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:55:34.538+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:55:34.538+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:55:34.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 23.154 seconds
[2024-08-20T23:56:04.788+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:56:04.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:56:04.835+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:56:04.832+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:56:08.307+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:56:08.473+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:56:08.493+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:56:08.590+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:56:08.589+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:56:08.614+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:56:08.614+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:56:08.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.855 seconds
[2024-08-20T23:56:38.768+0000] {processor.py:161} INFO - Started process (PID=214) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:56:38.769+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:56:38.771+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:56:38.771+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:56:41.717+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:56:41.937+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:56:41.964+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:56:42.179+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:56:42.165+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:56:42.209+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:56:42.208+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:56:42.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.484 seconds
[2024-08-20T23:57:12.394+0000] {processor.py:161} INFO - Started process (PID=222) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:57:12.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:57:12.397+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:57:12.396+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:57:19.797+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:57:19.977+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:57:19.991+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:57:20.031+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:57:20.031+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:57:20.054+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:57:20.054+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:57:20.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 7.683 seconds
[2024-08-20T23:58:04.044+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:58:04.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:58:04.048+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:58:04.047+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:58:35.392+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:58:34.194+0000] {timeout.py:68} ERROR - Process timed out, PID: 224
[2024-08-20T23:59:17.697+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:59:17.698+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:59:17.700+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:59:17.699+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:59:20.959+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:59:21.042+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:59:21.058+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:59:21.245+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:59:21.245+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:59:21.265+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:59:21.265+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:59:21.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.593 seconds
[2024-08-20T23:59:51.429+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/newspipeline.py
[2024-08-20T23:59:51.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-20T23:59:51.431+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:59:51.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:59:54.032+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-20T23:59:54.113+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-20T23:59:54.123+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-20T23:59:54.149+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:59:54.149+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-20T23:59:54.166+0000] {logging_mixin.py:188} INFO - [2024-08-20T23:59:54.166+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-20 00:00:00+00:00, run_after=2024-08-21 00:00:00+00:00
[2024-08-20T23:59:54.182+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.760 seconds
