[2024-08-21T00:00:33.104+0000] {processor.py:161} INFO - Started process (PID=254) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:00:33.107+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:00:33.108+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:00:33.107+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:01:03.302+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:01:03.176+0000] {timeout.py:68} ERROR - Process timed out, PID: 254
[2024-08-21T00:01:46.877+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:01:46.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:01:46.882+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:01:46.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:02:27.414+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:02:27.413+0000] {timeout.py:68} ERROR - Process timed out, PID: 263
[2024-08-21T00:02:27.840+0000] {logging_mixin.py:188} WARNING - Exception ignored in sys.unraisablehook: <built-in function unraisablehook>
[2024-08-21T00:02:27.840+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-08-21T00:02:27.840+0000] {logging_mixin.py:188} WARNING -   File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py", line 453, in _exit_gracefully
[2024-08-21T00:02:27.841+0000] {logging_mixin.py:188} WARNING -     self.log.debug("Current Stacktrace is: %s", "\n".join(map(str, inspect.stack())))
[2024-08-21T00:02:27.841+0000] {logging_mixin.py:188} WARNING -                                                                    ^^^^^^^^^^^^^^^
[2024-08-21T00:02:27.841+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/inspect.py", line 1771, in stack
[2024-08-21T00:02:27.842+0000] {logging_mixin.py:188} WARNING -     return getouterframes(sys._getframe(1), context)
[2024-08-21T00:02:27.842+0000] {logging_mixin.py:188} WARNING -            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-08-21T00:02:27.843+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/inspect.py", line 1746, in getouterframes
[2024-08-21T00:02:27.843+0000] {logging_mixin.py:188} WARNING -     traceback_info = getframeinfo(frame, context)
[2024-08-21T00:02:27.844+0000] {logging_mixin.py:188} WARNING -                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-08-21T00:02:27.844+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/inspect.py", line 1708, in getframeinfo
[2024-08-21T00:02:27.845+0000] {logging_mixin.py:188} WARNING -     lines, lnum = findsource(frame)
[2024-08-21T00:02:27.845+0000] {logging_mixin.py:188} WARNING -                   ^^^^^^^^^^^^^^^^^
[2024-08-21T00:02:27.846+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/inspect.py", line 1084, in findsource
[2024-08-21T00:02:27.846+0000] {logging_mixin.py:188} WARNING -     module = getmodule(object, file)
[2024-08-21T00:02:27.847+0000] {logging_mixin.py:188} WARNING -              ^^^^^^^^^^^^^^^^^^^^^^^
[2024-08-21T00:02:27.847+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/inspect.py", line 1010, in getmodule
[2024-08-21T00:02:27.847+0000] {logging_mixin.py:188} WARNING -     os.path.realpath(f)] = module.__name__
[2024-08-21T00:02:27.848+0000] {logging_mixin.py:188} WARNING -     ^^^^^^^^^^^^^^^^^^^
[2024-08-21T00:02:27.848+0000] {logging_mixin.py:188} WARNING -   File "<frozen posixpath>", line 427, in realpath
[2024-08-21T00:02:27.849+0000] {logging_mixin.py:188} WARNING -   File "<frozen posixpath>", line 462, in _joinrealpath
[2024-08-21T00:02:27.849+0000] {logging_mixin.py:188} WARNING -   File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py", line 457, in _exit_gracefully
[2024-08-21T00:02:27.849+0000] {logging_mixin.py:188} WARNING -     sys.exit(os.EX_OK)
[2024-08-21T00:02:27.850+0000] {logging_mixin.py:188} WARNING - SystemExit: 0
[2024-08-21T00:02:29.876+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:02:29.933+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:02:29.942+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:02:30.150+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:02:30.150+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:02:30.167+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:02:30.167+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:02:30.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 43.332 seconds
[2024-08-21T00:02:46.338+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:02:46.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:02:46.341+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:02:46.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:02:49.740+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:02:49.820+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:02:49.824+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:02:49.880+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:02:49.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:02:49.896+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:02:49.896+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:02:49.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.577 seconds
[2024-08-21T00:03:20.051+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:03:20.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:03:20.054+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:03:20.054+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:03:22.187+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:03:22.494+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:03:22.499+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:03:22.520+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:03:22.520+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:03:22.537+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:03:22.536+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:03:22.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.531 seconds
[2024-08-21T00:03:52.650+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:03:52.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:03:52.653+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:03:52.652+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:03:54.702+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:03:54.787+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:03:56.566+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:03:56.600+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:03:56.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:03:56.617+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:03:56.617+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:03:56.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.992 seconds
[2024-08-21T00:04:26.801+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:04:26.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:04:26.806+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:04:26.805+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:04:32.021+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:04:32.080+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:04:32.087+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:04:32.113+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:04:32.112+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:04:32.133+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:04:32.133+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:04:32.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.360 seconds
[2024-08-21T00:05:02.283+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:05:02.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:05:02.287+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:05:02.287+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:05:05.020+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:05:05.083+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:05:05.091+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:05:05.115+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:05:05.115+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:05:05.133+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:05:05.133+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:05:05.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.872 seconds
[2024-08-21T00:05:35.268+0000] {processor.py:161} INFO - Started process (PID=86) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:05:35.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:05:35.271+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:05:35.271+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:05:37.628+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:05:37.691+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:05:37.700+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:05:37.727+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:05:37.727+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:05:37.744+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:05:37.744+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:05:37.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.497 seconds
[2024-08-21T00:06:07.903+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:06:07.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:06:07.906+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:06:07.906+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:06:10.360+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:06:10.426+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:06:10.432+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:06:10.454+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:06:10.454+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:06:10.473+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:06:10.473+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:06:10.489+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.591 seconds
[2024-08-21T00:06:40.609+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:06:40.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:06:40.612+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:06:40.612+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:06:43.278+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:06:43.347+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:06:43.356+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:06:43.381+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:06:43.381+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:06:43.399+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:06:43.398+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:06:43.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.812 seconds
[2024-08-21T00:07:13.700+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:07:13.701+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:07:13.702+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:07:13.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:07:16.338+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:07:16.405+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:07:16.413+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:07:16.437+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:07:16.437+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:07:16.457+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:07:16.457+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:07:16.473+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.778 seconds
[2024-08-21T00:07:46.649+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:07:46.650+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:07:46.652+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:07:46.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:07:49.339+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:07:49.403+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:07:49.411+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:07:49.463+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:07:49.463+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:07:49.488+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:07:49.488+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:07:49.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.862 seconds
[2024-08-21T00:08:20.255+0000] {processor.py:161} INFO - Started process (PID=127) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:08:20.256+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:08:20.258+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:08:20.257+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:08:22.628+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:08:22.682+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:08:22.693+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:08:22.719+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:08:22.719+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:08:22.736+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:08:22.736+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:08:22.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.503 seconds
[2024-08-21T00:08:52.896+0000] {processor.py:161} INFO - Started process (PID=142) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:08:52.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:08:52.898+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:08:52.898+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:08:55.121+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:08:55.172+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:08:55.176+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:08:55.195+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:08:55.195+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:08:55.211+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:08:55.210+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:08:55.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.333 seconds
[2024-08-21T00:09:25.355+0000] {processor.py:161} INFO - Started process (PID=150) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:09:25.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:09:25.358+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:09:25.357+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:09:27.581+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:09:27.635+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:09:27.640+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:09:27.660+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:09:27.660+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:09:27.677+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:09:27.676+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:09:27.695+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.345 seconds
[2024-08-21T00:09:57.828+0000] {processor.py:161} INFO - Started process (PID=158) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:09:57.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:09:57.831+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:09:57.830+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:10:00.045+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:10:00.096+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:10:00.101+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:10:00.120+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:10:00.119+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:10:00.137+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:10:00.137+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:10:00.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.331 seconds
[2024-08-21T00:10:30.297+0000] {processor.py:161} INFO - Started process (PID=166) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:10:30.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:10:30.302+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:10:30.301+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:10:33.471+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:10:33.530+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:10:33.539+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:10:33.568+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:10:33.568+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:10:33.588+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:10:33.588+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:10:33.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.315 seconds
[2024-08-21T00:11:03.762+0000] {processor.py:161} INFO - Started process (PID=174) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:11:03.764+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:11:03.766+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:11:03.766+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:11:08.623+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:11:09.036+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:11:09.074+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:11:09.610+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:11:09.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:11:09.822+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:11:09.814+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:11:09.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 6.158 seconds
[2024-08-21T00:11:52.194+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:11:52.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:11:52.213+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:11:52.202+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:14:12.133+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:14:12.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:14:12.171+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:14:12.166+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:15:06.301+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:15:06.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:15:06.303+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:15:06.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:15:09.215+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:15:09.301+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:15:09.403+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:15:09.560+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:15:09.559+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:15:09.592+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:15:09.591+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:15:09.612+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.318 seconds
[2024-08-21T00:15:42.037+0000] {processor.py:161} INFO - Started process (PID=201) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:15:42.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:15:42.045+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:15:42.045+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:16:20.036+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:16:17.057+0000] {timeout.py:68} ERROR - Process timed out, PID: 201
[2024-08-21T00:17:42.668+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:17:42.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:17:42.674+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:17:42.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:18:14.088+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:18:14.087+0000] {timeout.py:68} ERROR - Process timed out, PID: 203
[2024-08-21T00:24:16.350+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:24:16.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:24:16.628+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:24:16.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:25:05.072+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:25:05.072+0000] {timeout.py:68} ERROR - Process timed out, PID: 66
[2024-08-21T00:25:43.558+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:25:43.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:25:43.567+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:25:43.567+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:27:27.302+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:27:27.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:27:29.129+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:27:27.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:28:36.546+0000] {processor.py:161} INFO - Started process (PID=88) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:28:37.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:28:38.707+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:28:38.706+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:29:19.085+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:29:18.125+0000] {timeout.py:68} ERROR - Process timed out, PID: 88
[2024-08-21T00:29:53.632+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:29:53.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:29:53.645+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:29:53.644+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:30:12.789+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:30:13.127+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:30:13.141+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:30:13.402+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:30:13.402+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:30:13.445+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:30:13.445+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:30:13.502+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 19.920 seconds
[2024-08-21T00:30:43.761+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:30:43.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:30:43.764+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:30:43.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:30:46.567+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:30:47.094+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:30:47.103+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:30:47.133+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:30:47.133+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:30:47.155+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:30:47.154+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:30:47.173+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.417 seconds
[2024-08-21T00:31:17.301+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:31:17.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:31:17.304+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:31:17.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:31:20.164+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:31:20.660+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:31:20.676+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:31:20.713+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:31:20.712+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:31:20.754+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:31:20.754+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:31:20.772+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.477 seconds
[2024-08-21T00:31:50.927+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:31:50.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:31:50.930+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:31:50.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:31:53.467+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:31:53.756+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:31:53.767+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:31:53.802+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:31:53.801+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:31:53.820+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:31:53.820+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:31:53.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.920 seconds
[2024-08-21T00:32:23.982+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:32:23.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:32:23.986+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:32:23.984+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:32:26.890+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:32:27.270+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:32:27.282+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:32:27.319+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:32:27.319+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:32:27.337+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:32:27.337+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:32:27.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.381 seconds
[2024-08-21T00:32:57.492+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:32:57.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:32:57.495+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:32:57.494+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:00.134+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:33:00.367+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:33:00.379+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:00.412+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:00.412+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:33:00.432+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:00.431+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:33:00.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.963 seconds
[2024-08-21T00:33:30.585+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:30.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:33:30.588+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:30.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:35.853+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:33:36.225+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:33:36.243+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:36.280+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:36.280+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:33:36.298+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:36.298+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:33:36.315+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.738 seconds
[2024-08-21T00:33:44.644+0000] {processor.py:161} INFO - Started process (PID=163) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:44.645+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:33:44.647+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:44.646+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:50.285+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:33:50.407+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:33:50.430+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:50.410+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 179, in <module>
    extract_task >> upload_task >> upload_task
                    ~~~~~~~~~~~~^^~~~~~~
NameError: name 'upload_taskload_task' is not defined
[2024-08-21T00:33:50.438+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:50.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.915 seconds
[2024-08-21T00:33:54.521+0000] {processor.py:161} INFO - Started process (PID=165) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:33:54.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:33:54.524+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:33:54.523+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:34:33.982+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:34:30.125+0000] {timeout.py:68} ERROR - Process timed out, PID: 165
[2024-08-21T00:39:01.093+0000] {processor.py:161} INFO - Started process (PID=173) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:39:01.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:39:01.099+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:39:01.095+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:39:31.132+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:39:31.114+0000] {timeout.py:68} ERROR - Process timed out, PID: 173
[2024-08-21T00:43:56.059+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:43:56.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:43:56.099+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:43:56.099+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:44:13.378+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:44:13.656+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:44:13.675+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:44:14.106+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:44:14.106+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:44:14.132+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:44:14.131+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:44:14.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 18.108 seconds
[2024-08-21T00:44:44.348+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:44:44.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:44:44.357+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:44:44.353+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:44:50.226+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:44:50.822+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:44:50.832+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:44:50.867+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:44:50.866+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:44:50.888+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:44:50.888+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:44:50.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 6.574 seconds
[2024-08-21T00:45:22.659+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:45:22.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:45:22.699+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:45:22.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:47:12.213+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:47:12.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:47:12.240+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:47:12.237+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:47:44.586+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:47:44.562+0000] {timeout.py:68} ERROR - Process timed out, PID: 72
[2024-08-21T00:48:27.037+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:48:27.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:48:27.043+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:48:27.040+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:48:38.645+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:48:38.999+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:48:39.020+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:48:39.177+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:48:39.176+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:48:39.194+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:48:39.194+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:48:39.211+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 12.190 seconds
[2024-08-21T00:49:10.170+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:49:10.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:49:10.174+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:49:10.172+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:49:14.108+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:49:14.885+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:49:14.910+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:49:15.146+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:49:15.145+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:49:15.167+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:49:15.167+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:49:15.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.032 seconds
[2024-08-21T00:49:50.266+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:49:50.268+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:49:50.273+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:49:50.269+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:49:57.511+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:49:58.025+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:49:58.046+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:49:58.481+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:49:58.475+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:49:58.518+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:49:58.518+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:49:58.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 8.368 seconds
[2024-08-21T00:50:29.003+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:50:29.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:50:29.005+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:50:29.005+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:50:59.025+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:50:59.009+0000] {timeout.py:68} ERROR - Process timed out, PID: 111
[2024-08-21T00:50:59.588+0000] {processor.py:161} INFO - Started process (PID=113) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:50:59.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:50:59.591+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:50:59.591+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:51:33.445+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:51:31.979+0000] {timeout.py:68} ERROR - Process timed out, PID: 113
[2024-08-21T00:52:24.921+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:52:24.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:52:24.937+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:52:24.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:52:58.567+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:52:58.530+0000] {timeout.py:68} ERROR - Process timed out, PID: 120
[2024-08-21T00:52:58.568+0000] {logging_mixin.py:188} WARNING - Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7f316b50de40>
[2024-08-21T00:52:58.569+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-08-21T00:52:58.569+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/weakref.py", line 369, in remove
[2024-08-21T00:52:58.582+0000] {logging_mixin.py:188} WARNING -     def remove(k, selfref=ref(self)):
[2024-08-21T00:52:58.583+0000] {logging_mixin.py:188} WARNING -   File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
[2024-08-21T00:52:58.595+0000] {logging_mixin.py:188} WARNING -     raise AirflowTaskTimeout(self.error_message)
[2024-08-21T00:52:58.596+0000] {logging_mixin.py:188} WARNING - airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/newspipeline.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.9.2/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.9.2/best-practices.html#reducing-dag-complexity, PID: 120
[2024-08-21T00:53:20.116+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:53:20.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:53:20.119+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:20.118+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:53:25.538+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:53:26.091+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:53:26.104+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:53:26.123+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:26.123+0000] {taskinstance.py:2907} ERROR - {'DAG Id': 'etl_data_pipeline', 'Task Id': 'load_data_to_postgres', 'Run Id': 'manual__2024-08-21T00:49:30.528564+00:00', 'Hostname': '986f1fc413c3', 'External Executor Id': '58e42a12-0417-4ad9-b180-a39722123bed'}
[2024-08-21T00:53:26.143+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:26.142+0000] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_data_pipeline, task_id=load_data_to_postgres, run_id=manual__2024-08-21T00:49:30.528564+00:00, execution_date=20240821T004930, start_date=20240821T005032, end_date=20240821T005326
[2024-08-21T00:53:26.212+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: etl_data_pipeline.load_data_to_postgres manual__2024-08-21T00:49:30.528564+00:00 [up_for_retry]> in state up_for_retry
[2024-08-21T00:53:26.214+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:26.213+0000] {taskinstance.py:2907} ERROR - {'DAG Id': 'etl_data_pipeline', 'Task Id': 'load_data_to_postgres', 'Run Id': 'manual__2024-08-21T00:49:30.528564+00:00', 'Hostname': '986f1fc413c3', 'External Executor Id': '58e42a12-0417-4ad9-b180-a39722123bed'}
[2024-08-21T00:53:26.219+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:26.219+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=etl_data_pipeline, task_id=load_data_to_postgres, run_id=manual__2024-08-21T00:49:30.528564+00:00, execution_date=20240821T004930, start_date=20240821T005032, end_date=20240821T005326
[2024-08-21T00:53:26.221+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: etl_data_pipeline.load_data_to_postgres manual__2024-08-21T00:49:30.528564+00:00 [failed]> in state failed
[2024-08-21T00:53:26.362+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:26.362+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:53:26.377+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:26.377+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:53:26.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 6.325 seconds
[2024-08-21T00:53:58.651+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:53:58.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:53:58.768+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:53:58.768+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:54:37.051+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:54:33.232+0000] {timeout.py:68} ERROR - Process timed out, PID: 131
[2024-08-21T00:55:19.212+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:55:19.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:55:19.561+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:55:19.560+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:56:40.977+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:56:41.053+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:56:41.110+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:56:41.110+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:57:12.710+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:57:12.632+0000] {timeout.py:68} ERROR - Process timed out, PID: 153
[2024-08-21T00:57:45.799+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:57:45.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:57:45.832+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:57:45.830+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:58:16.189+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:58:16.186+0000] {timeout.py:68} ERROR - Process timed out, PID: 161
[2024-08-21T00:58:55.731+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:58:55.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:58:55.924+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:58:55.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:59:09.507+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T00:59:09.681+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T00:59:09.701+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:59:09.977+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:59:09.976+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T00:59:09.996+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:59:09.995+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T00:59:10.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 14.563 seconds
[2024-08-21T00:59:42.259+0000] {processor.py:161} INFO - Started process (PID=184) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T00:59:42.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T00:59:42.261+0000] {logging_mixin.py:188} INFO - [2024-08-21T00:59:42.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T00:59:59.865+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:00:00.088+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:00:00.099+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:00:00.127+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:00:00.127+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:00:00.149+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:00:00.149+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:00:00.169+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 18.502 seconds
[2024-08-21T01:00:30.533+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:00:30.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:00:30.536+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:00:30.535+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:00:34.126+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:00:34.224+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:00:34.233+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:00:34.262+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:00:34.261+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:00:34.281+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:00:34.281+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:00:34.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.779 seconds
[2024-08-21T01:01:04.456+0000] {processor.py:161} INFO - Started process (PID=205) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:01:04.457+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:01:04.460+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:01:04.458+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:01:07.126+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:01:07.248+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:01:07.260+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:01:07.294+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:01:07.294+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:01:07.316+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:01:07.316+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:01:07.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.887 seconds
[2024-08-21T01:01:37.423+0000] {processor.py:161} INFO - Started process (PID=213) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:01:37.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:01:37.425+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:01:37.425+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:01:40.455+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:01:40.532+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:01:40.542+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:01:40.568+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:01:40.568+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:01:40.588+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:01:40.588+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:01:40.603+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.186 seconds
[2024-08-21T01:02:10.901+0000] {processor.py:161} INFO - Started process (PID=221) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:02:10.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:02:10.903+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:02:10.902+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:02:14.039+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:02:14.191+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:02:14.202+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:02:14.232+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:02:14.232+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:02:14.268+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:02:14.267+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:02:14.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.391 seconds
[2024-08-21T01:02:56.451+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:02:56.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:02:56.454+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:02:56.454+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:04:19.992+0000] {processor.py:161} INFO - Started process (PID=230) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:04:19.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:04:19.994+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:04:19.993+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:04:23.146+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:04:23.687+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:04:23.711+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:04:23.879+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:04:23.878+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:04:23.900+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:04:23.900+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:04:23.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.959 seconds
[2024-08-21T01:04:56.270+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:04:56.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:04:57.038+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:04:56.821+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:06:55.354+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:06:55.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:06:55.356+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:06:55.356+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:06:58.535+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:06:58.932+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:06:58.959+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:06:59.164+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:06:59.163+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:06:59.192+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:06:59.192+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:06:59.213+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.868 seconds
[2024-08-21T01:07:58.424+0000] {processor.py:161} INFO - Started process (PID=254) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:07:58.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:07:58.443+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:07:58.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:08:31.694+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:08:31.465+0000] {timeout.py:68} ERROR - Process timed out, PID: 254
[2024-08-21T01:09:23.125+0000] {processor.py:161} INFO - Started process (PID=256) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:09:23.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:09:27.740+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:09:27.320+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:10:36.637+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:10:07.323+0000] {timeout.py:68} ERROR - Process timed out, PID: 256
[2024-08-21T01:12:15.012+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:12:15.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:12:15.015+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:12:15.014+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:12:19.589+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:12:20.022+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:12:21.125+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:12:27.943+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:12:27.905+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:12:28.003+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:12:28.003+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:12:28.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 13.027 seconds
[2024-08-21T01:12:58.207+0000] {processor.py:161} INFO - Started process (PID=271) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:12:58.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:12:58.210+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:12:58.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:13:01.820+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:13:01.927+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-08-21T01:13:01.954+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:13:02.010+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:13:02.009+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-08-21T01:13:02.042+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:13:02.042+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-08-21 00:00:00+00:00, run_after=2024-08-22 00:00:00+00:00
[2024-08-21T01:13:02.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.873 seconds
[2024-08-21T01:13:33.719+0000] {processor.py:161} INFO - Started process (PID=279) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:13:34.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:13:34.675+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:13:34.328+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:14:07.618+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:14:07.601+0000] {timeout.py:68} ERROR - Process timed out, PID: 279
[2024-08-21T01:14:39.060+0000] {processor.py:161} INFO - Started process (PID=287) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:14:39.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:14:39.065+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:14:39.062+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:15:10.564+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:15:09.234+0000] {timeout.py:68} ERROR - Process timed out, PID: 287
[2024-08-21T01:16:07.564+0000] {processor.py:161} INFO - Started process (PID=295) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:16:07.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:16:07.568+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:16:07.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:16:10.627+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-08-21T01:16:21.450+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:16:20.776+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    load_data_to_postgres(df, table_name, conn_string)
  File "/opt/airflow/dags/newspipeline.py", line 63, in load_data_to_postgres
    conn = psycopg2.connect(conn_string)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution
[2024-08-21T01:16:21.512+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-08-21T01:16:35.477+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 843, in process_file
    DagFileProcessor.update_import_errors(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 622, in update_import_errors
    existing_import_error_files = [x.filename for x in session.query(errors.ImportError.filename).all()]
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-08-21T01:17:12.858+0000] {processor.py:161} INFO - Started process (PID=309) to work on /opt/airflow/dags/newspipeline.py
[2024-08-21T01:17:12.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-08-21T01:17:12.860+0000] {logging_mixin.py:188} INFO - [2024-08-21T01:17:12.859+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
