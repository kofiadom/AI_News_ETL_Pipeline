[2024-09-06T05:17:06.194+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T05:17:06.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T05:17:06.199+0000] {logging_mixin.py:188} INFO - [2024-09-06T05:17:06.198+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T05:17:13.992+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T05:17:20.646+0000] {logging_mixin.py:188} INFO - Successfully loaded data into news_articles table
[2024-09-06T05:17:21.479+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T05:17:26.476+0000] {logging_mixin.py:188} INFO - [2024-09-06T05:17:26.475+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T05:17:26.662+0000] {logging_mixin.py:188} INFO - [2024-09-06T05:17:26.662+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T05:17:26.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 20.643 seconds
[2024-09-06T05:18:07.669+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T05:18:07.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T05:18:09.609+0000] {logging_mixin.py:188} INFO - [2024-09-06T05:18:09.117+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T05:18:40.571+0000] {logging_mixin.py:188} INFO - [2024-09-06T05:18:40.565+0000] {timeout.py:68} ERROR - Process timed out, PID: 70
[2024-09-06T06:56:56.156+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T06:56:56.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T06:56:56.177+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:56:56.177+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:57:11.347+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T06:57:11.391+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:57:11.348+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T06:57:11.392+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:57:11.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 15.287 seconds
[2024-09-06T06:57:41.606+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T06:57:41.607+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T06:57:41.611+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:57:41.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:57:44.347+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T06:57:44.353+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:57:44.348+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T06:57:44.369+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:57:44.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.795 seconds
[2024-09-06T06:58:14.540+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T06:58:14.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T06:58:14.543+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:58:14.543+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:58:16.989+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T06:58:17.008+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:58:16.990+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T06:58:17.010+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:58:17.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.529 seconds
[2024-09-06T06:58:47.192+0000] {processor.py:161} INFO - Started process (PID=85) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T06:58:47.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T06:58:47.195+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:58:47.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:58:49.916+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T06:58:49.929+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:58:49.916+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T06:58:49.930+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:58:49.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.785 seconds
[2024-09-06T06:59:20.028+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T06:59:20.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T06:59:20.031+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:59:20.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:59:22.456+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T06:59:22.460+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:59:22.457+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T06:59:22.461+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:59:22.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.478 seconds
[2024-09-06T06:59:52.637+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T06:59:52.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T06:59:52.639+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:59:52.639+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:59:57.047+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T06:59:57.056+0000] {logging_mixin.py:188} INFO - [2024-09-06T06:59:57.047+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T06:59:57.057+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T06:59:57.083+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.452 seconds
[2024-09-06T07:00:27.219+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:00:27.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:00:27.221+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:00:27.221+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:00:29.635+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:00:29.654+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:00:29.636+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:00:29.656+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:00:29.703+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.489 seconds
[2024-09-06T07:00:59.846+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:00:59.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:00:59.849+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:00:59.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:01:05.942+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:01:05.948+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:01:05.942+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:01:05.982+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:01:06.008+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 6.168 seconds
[2024-09-06T07:01:36.130+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:01:36.131+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:01:36.133+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:01:36.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:01:38.780+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:01:38.786+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:01:38.781+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:01:38.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:01:38.815+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.693 seconds
[2024-09-06T07:02:09.024+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:02:09.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:02:09.047+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:02:09.046+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:02:12.302+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:02:12.309+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:02:12.303+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:02:12.311+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:02:12.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.327 seconds
[2024-09-06T07:02:42.537+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:02:42.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:02:42.542+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:02:42.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:02:45.436+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:02:45.445+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:02:45.438+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:02:45.447+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:02:45.509+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.982 seconds
[2024-09-06T07:03:17.864+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:03:21.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:03:28.736+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:03:28.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:04:51.227+0000] {processor.py:161} INFO - Started process (PID=164) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:04:51.229+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:04:51.233+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:04:51.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:04:54.376+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:04:54.384+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:04:54.377+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:04:54.386+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:04:54.436+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.218 seconds
[2024-09-06T07:05:25.162+0000] {processor.py:161} INFO - Started process (PID=172) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:05:25.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:05:25.307+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:05:25.270+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:06:46.111+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:06:46.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:06:46.115+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:06:46.114+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:06:48.739+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:06:48.747+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:06:48.739+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:06:48.748+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:06:48.779+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.677 seconds
[2024-09-06T07:07:18.952+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:07:18.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:07:18.956+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:07:18.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:07:21.375+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:07:21.381+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:07:21.378+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:07:21.382+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:07:21.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.468 seconds
[2024-09-06T07:07:51.536+0000] {processor.py:161} INFO - Started process (PID=198) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:07:51.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:07:51.539+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:07:51.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:07:55.016+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:07:55.021+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:07:55.017+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:07:55.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:07:55.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.519 seconds
[2024-09-06T07:09:19.146+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:09:19.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:09:19.192+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:09:19.192+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:09:21.298+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:09:21.314+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:09:21.311+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:09:21.316+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:09:21.376+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.235 seconds
[2024-09-06T07:09:51.445+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:09:51.446+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:09:51.450+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:09:51.447+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:09:56.057+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:09:56.064+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:09:56.058+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:09:56.065+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:09:56.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.642 seconds
[2024-09-06T07:10:26.230+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:10:26.232+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:10:26.234+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:10:26.233+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:10:28.237+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:10:28.240+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:10:28.237+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:10:28.241+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:10:28.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.039 seconds
[2024-09-06T07:10:58.399+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:10:58.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:10:58.452+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:10:58.452+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:11:01.007+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:11:01.010+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:11:01.008+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:11:01.012+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:11:01.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.633 seconds
[2024-09-06T07:11:31.096+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:11:31.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:11:31.099+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:11:31.099+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:11:34.001+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:11:34.009+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:11:34.002+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:11:34.010+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:11:34.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.937 seconds
[2024-09-06T07:12:04.178+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:12:04.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:12:04.181+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:12:04.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:12:07.321+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:12:07.326+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:12:07.321+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:12:07.327+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:12:07.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.180 seconds
[2024-09-06T07:12:37.555+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:12:37.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:12:37.558+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:12:37.558+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:12:40.175+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:12:40.179+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:12:40.176+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:12:40.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:12:40.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.649 seconds
[2024-09-06T07:13:10.367+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:13:10.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:13:10.369+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:13:10.369+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:13:12.700+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:13:12.706+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:13:12.701+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:13:12.707+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:13:12.725+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.363 seconds
[2024-09-06T07:13:42.788+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:13:42.789+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:13:42.791+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:13:42.790+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:13:45.706+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:13:45.712+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:13:45.707+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:13:45.713+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:13:45.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.949 seconds
[2024-09-06T07:14:15.889+0000] {processor.py:161} INFO - Started process (PID=118) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:14:15.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:14:15.891+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:14:15.891+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:14:18.385+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:14:18.398+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:14:18.385+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:14:18.399+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:14:18.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.534 seconds
[2024-09-06T07:14:48.586+0000] {processor.py:161} INFO - Started process (PID=126) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:14:48.587+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:14:48.588+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:14:48.588+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:14:50.888+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:14:50.900+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:14:50.888+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:14:50.901+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:14:51.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.454 seconds
[2024-09-06T07:15:21.187+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:15:21.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:15:21.190+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:15:21.190+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:15:23.839+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:15:23.847+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:15:23.840+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:15:23.956+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:15:23.975+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.794 seconds
[2024-09-06T07:15:54.123+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:15:54.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:15:54.127+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:15:54.126+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:05.485+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:16:05.492+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:05.486+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:16:05.493+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:05.514+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 11.397 seconds
[2024-09-06T07:16:35.644+0000] {processor.py:161} INFO - Started process (PID=155) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:35.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:16:35.648+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:35.647+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:38.198+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:16:38.202+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:38.198+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 102, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:16:38.204+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:38.225+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.588 seconds
[2024-09-06T07:16:43.399+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:43.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:16:43.402+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:43.402+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:45.657+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:16:45.746+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:45.742+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 104, in <module>
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:16:45.748+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:45.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.369 seconds
[2024-09-06T07:16:48.441+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:48.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:16:48.444+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:48.444+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:52.826+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:16:52.833+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:16:52.827+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:16:52.836+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:16:52.853+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 4.418 seconds
[2024-09-06T07:17:02.880+0000] {processor.py:161} INFO - Started process (PID=167) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:02.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:17:02.882+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:02.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:05.288+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:17:05.303+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:05.289+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:17:05.305+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:05.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.452 seconds
[2024-09-06T07:17:06.372+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:06.373+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:17:06.375+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:06.375+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:08.516+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:17:08.519+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:08.517+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:17:08.521+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:08.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.170 seconds
[2024-09-06T07:17:14.416+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:14.417+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:17:14.419+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:14.419+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:17.184+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:17:17.190+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:17.184+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:17:17.192+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:17.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.795 seconds
[2024-09-06T07:17:47.356+0000] {processor.py:161} INFO - Started process (PID=179) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:47.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:17:47.360+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:47.359+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:49.509+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:17:49.516+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:49.510+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:17:49.518+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:49.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.184 seconds
[2024-09-06T07:17:55.405+0000] {processor.py:161} INFO - Started process (PID=187) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:55.406+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:17:55.408+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:55.407+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:57.332+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:17:57.338+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:57.333+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 108, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:17:57.340+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:57.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 1.975 seconds
[2024-09-06T07:17:58.431+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:17:58.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:17:58.434+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:17:58.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:00.551+0000] {logging_mixin.py:188} INFO - Successfully saved 100 articles to ai_news_articles.json and ai_news_articles.csv
[2024-09-06T07:18:00.559+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:00.552+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 108, in <module>
    df = extract_data()
         ^^^^^^^^^^^^^^
  File "/opt/airflow/dags/newspipeline.py", line 55, in extract_data
    kwargs['ti'].xcom_push(key='extract_data', value=df)
    ~~~~~~^^^^^^
KeyError: 'ti'
[2024-09-06T07:18:00.560+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:00.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.158 seconds
[2024-09-06T07:18:02.628+0000] {processor.py:161} INFO - Started process (PID=191) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:02.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:18:02.631+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:02.630+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:02.644+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:02.643+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 107
    '''
    ^
SyntaxError: unterminated triple-quoted string literal (detected at line 189)
[2024-09-06T07:18:02.645+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:02.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.046 seconds
[2024-09-06T07:18:08.673+0000] {processor.py:161} INFO - Started process (PID=192) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:08.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:18:08.676+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:08.675+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:08.713+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:08.971+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:08.971+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:etl_data_pipeline
[2024-09-06T07:18:08.980+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:08.980+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:etl_data_pipeline
[2024-09-06T07:18:08.988+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:08.987+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:etl_data_pipeline
[2024-09-06T07:18:08.988+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:08.988+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:18:08.998+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:08.998+0000] {dag.py:3118} INFO - Creating ORM DAG for etl_data_pipeline
[2024-09-06T07:18:09.009+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:09.008+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:18:09.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.359 seconds
[2024-09-06T07:18:28.003+0000] {processor.py:161} INFO - Started process (PID=199) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:28.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:18:28.008+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:28.007+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:28.037+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:28.048+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:28.048+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:18:28.071+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:28.070+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:18:28.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.103 seconds
[2024-09-06T07:18:58.206+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:58.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:18:58.209+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:58.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:58.228+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:18:58.259+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:58.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:18:58.282+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:18:58.281+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:18:58.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.099 seconds
[2024-09-06T07:19:12.370+0000] {processor.py:161} INFO - Started process (PID=207) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:19:12.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:19:12.373+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:19:12.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:19:12.397+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:19:12.429+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:19:12.429+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:19:12.452+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:19:12.452+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:19:12.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.109 seconds
[2024-09-06T07:19:42.659+0000] {processor.py:161} INFO - Started process (PID=214) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:19:42.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:19:42.673+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:19:42.672+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:19:42.706+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:19:42.740+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:19:42.740+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:19:42.765+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:19:42.764+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:19:42.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.131 seconds
[2024-09-06T07:20:12.971+0000] {processor.py:161} INFO - Started process (PID=221) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:20:12.972+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:20:12.974+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:20:12.974+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:20:12.991+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:20:13.028+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:20:13.028+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:20:13.050+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:20:13.050+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:20:13.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.104 seconds
[2024-09-06T07:20:43.192+0000] {processor.py:161} INFO - Started process (PID=228) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:20:43.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:20:43.205+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:20:43.205+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:20:43.223+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:20:43.252+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:20:43.251+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:20:43.275+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:20:43.275+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-05 00:00:00+00:00, run_after=2024-09-06 00:00:00+00:00
[2024-09-06T07:20:43.295+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.109 seconds
[2024-09-06T07:21:13.493+0000] {processor.py:161} INFO - Started process (PID=235) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:21:13.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:21:13.498+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:21:13.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:21:13.534+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:21:13.951+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:21:13.946+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:21:13.982+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:21:13.982+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:21:14.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.521 seconds
[2024-09-06T07:21:44.283+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:21:44.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:21:44.354+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:21:44.328+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:21:44.713+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:22:18.335+0000] {processor.py:186} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 664, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 435, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 368, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 410, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py", line 183, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 677, in _sync_to_db
    _serialize_dag_capturing_errors(dag, session, processor_subdir)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 642, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/serialized_dag.py", line 158, in write_dag
    if session.scalar(
       ^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1747, in scalar
    return self.execute(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
           ^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-09-06T07:23:47.855+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:23:47.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:23:47.858+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:47.858+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:23:47.885+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:23:47.941+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:47.941+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:23:47.962+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:47.962+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:23:47.981+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.131 seconds
[2024-09-06T07:23:57.966+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:23:57.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:23:57.969+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:57.969+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:23:57.990+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:23:58.012+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:58.012+0000] {taskinstance.py:2907} ERROR - {'DAG Id': 'etl_data_pipeline', 'Task Id': 'load_data_to_postgres', 'Run Id': 'scheduled__2024-09-05T00:00:00+00:00', 'Hostname': 'ca47fb49acc7', 'External Executor Id': 'f4fa5a78-e26e-4f33-a573-91c891220735'}
[2024-09-06T07:23:58.031+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:58.031+0000] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_data_pipeline, task_id=load_data_to_postgres, run_id=scheduled__2024-09-05T00:00:00+00:00, execution_date=20240905T000000, start_date=20240906T072119, end_date=20240906T072358
[2024-09-06T07:23:58.039+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: etl_data_pipeline.load_data_to_postgres scheduled__2024-09-05T00:00:00+00:00 [up_for_retry]> in state up_for_retry
[2024-09-06T07:23:58.063+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:58.063+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:23:58.080+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:23:58.080+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:23:58.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.136 seconds
[2024-09-06T07:24:28.516+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:24:28.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:24:28.519+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:24:28.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:24:28.772+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:24:28.805+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:24:28.805+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:24:28.826+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:24:28.826+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:24:28.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.336 seconds
[2024-09-06T07:24:59.270+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:24:59.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:24:59.311+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:24:59.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:24:59.340+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:24:59.372+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:24:59.372+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:24:59.395+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:24:59.395+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:24:59.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.148 seconds
[2024-09-06T07:25:29.540+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:25:29.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:25:29.542+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:25:29.542+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:25:29.561+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:25:29.599+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:25:29.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:25:29.621+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:25:29.621+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:25:29.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.105 seconds
[2024-09-06T07:26:00.631+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:26:00.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:26:00.634+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:26:00.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:26:00.653+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:26:00.684+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:26:00.683+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:26:00.709+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:26:00.709+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:26:00.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.105 seconds
[2024-09-06T07:26:30.831+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:26:30.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:26:30.833+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:26:30.833+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:26:30.852+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:26:30.884+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:26:30.884+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:26:30.907+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:26:30.906+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:26:30.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.106 seconds
[2024-09-06T07:27:00.973+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:27:00.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:27:00.976+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:27:00.976+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:27:00.999+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:27:01.033+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:27:01.032+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:27:01.055+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:27:01.055+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:27:01.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.108 seconds
[2024-09-06T07:27:31.220+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:27:31.221+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:27:31.223+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:27:31.222+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:27:31.243+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:27:31.274+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:27:31.274+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:27:31.297+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:27:31.296+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:27:31.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.099 seconds
[2024-09-06T07:28:01.364+0000] {processor.py:161} INFO - Started process (PID=101) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:28:01.365+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:28:01.367+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:28:01.367+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:28:01.386+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:28:01.415+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:28:01.415+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:28:01.438+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:28:01.437+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:28:01.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.100 seconds
[2024-09-06T07:28:31.575+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:28:31.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:28:31.577+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:28:31.577+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:28:31.598+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:28:31.630+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:28:31.630+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:28:31.653+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:28:31.653+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:28:31.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.102 seconds
[2024-09-06T07:29:01.842+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:29:01.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:29:01.845+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:29:01.844+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:29:01.873+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:29:01.915+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:29:01.915+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:29:01.940+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:29:01.940+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:29:01.964+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.130 seconds
[2024-09-06T07:29:32.095+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:29:32.096+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:29:32.097+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:29:32.097+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:29:32.116+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:29:32.148+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:29:32.147+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:29:32.171+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:29:32.170+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:29:32.190+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.101 seconds
[2024-09-06T07:30:02.247+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:30:02.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:30:02.250+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:30:02.250+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:30:02.270+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:30:02.333+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:30:02.333+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:30:02.358+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:30:02.358+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:30:02.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.144 seconds
[2024-09-06T07:30:32.526+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:30:32.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:30:32.528+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:30:32.528+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:30:32.548+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:30:32.579+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:30:32.579+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:30:32.601+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:30:32.601+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:30:32.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.099 seconds
[2024-09-06T07:31:05.869+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:05.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:31:05.871+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:05.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:05.900+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:05.937+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:05.937+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:31:05.960+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:05.960+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:31:06.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.158 seconds
[2024-09-06T07:31:18.954+0000] {processor.py:161} INFO - Started process (PID=146) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:18.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:31:18.958+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:18.958+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:18.985+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:19.016+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:19.015+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:31:19.037+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:19.036+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:31:19.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.112 seconds
[2024-09-06T07:31:49.178+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:49.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:31:49.182+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:49.181+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:49.200+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:49.231+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:49.231+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:31:49.254+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:49.254+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:31:49.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.100 seconds
[2024-09-06T07:31:57.231+0000] {processor.py:161} INFO - Started process (PID=160) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:57.232+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:31:57.234+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:57.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:57.257+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:31:57.286+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:57.286+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:31:57.310+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:31:57.309+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:31:57.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.105 seconds
[2024-09-06T07:32:27.454+0000] {processor.py:161} INFO - Started process (PID=167) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:32:27.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:32:27.457+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:32:27.456+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:32:27.478+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:32:27.511+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:32:27.511+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:32:27.534+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:32:27.534+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:32:27.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.106 seconds
[2024-09-06T07:32:58.139+0000] {processor.py:161} INFO - Started process (PID=174) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:32:58.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:32:58.144+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:32:58.142+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:32:58.179+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:32:58.497+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:32:58.493+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:32:58.541+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:32:58.540+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:32:58.579+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.453 seconds
[2024-09-06T07:33:28.764+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:33:28.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:33:28.766+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:33:28.766+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:33:28.791+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:33:28.823+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:33:28.823+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:33:28.844+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:33:28.844+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:33:28.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.104 seconds
[2024-09-06T07:33:59.035+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:33:59.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:33:59.039+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:33:59.037+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:33:59.059+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:33:59.094+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:33:59.094+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:33:59.115+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:33:59.115+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:33:59.131+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.102 seconds
[2024-09-06T07:34:29.232+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:34:29.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:34:29.234+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:34:29.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:34:29.252+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:34:29.286+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:34:29.286+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:34:29.306+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:34:29.306+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:34:29.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.099 seconds
[2024-09-06T07:35:11.907+0000] {processor.py:161} INFO - Started process (PID=197) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:35:15.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:35:15.261+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:35:15.255+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:35:15.683+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:35:48.214+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:35:48.202+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:35:48.262+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:35:48.262+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:35:48.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 38.887 seconds
[2024-09-06T07:36:18.480+0000] {processor.py:161} INFO - Started process (PID=204) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:36:18.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:36:18.484+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:36:18.483+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:36:18.549+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:36:18.626+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:36:18.625+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:36:18.647+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:36:18.647+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:36:18.666+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.195 seconds
[2024-09-06T07:36:48.917+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:36:48.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:36:48.920+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:36:48.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:36:48.938+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:36:48.974+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:36:48.974+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:36:48.998+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:36:48.997+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:36:49.019+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.108 seconds
[2024-09-06T07:37:19.185+0000] {processor.py:161} INFO - Started process (PID=218) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:37:19.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:37:19.188+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:37:19.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:37:19.210+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:37:19.248+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:37:19.247+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:37:19.271+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:37:19.271+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:37:19.288+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.110 seconds
[2024-09-06T07:37:49.400+0000] {processor.py:161} INFO - Started process (PID=225) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:37:49.401+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:37:49.403+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:37:49.403+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:37:49.423+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:37:49.453+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:37:49.453+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:37:49.474+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:37:49.473+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:37:49.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.123 seconds
[2024-09-06T07:38:19.637+0000] {processor.py:161} INFO - Started process (PID=233) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:38:19.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:38:19.640+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:38:19.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:38:19.660+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:38:19.695+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:38:19.695+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:38:19.717+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:38:19.717+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:38:19.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.111 seconds
[2024-09-06T07:38:49.807+0000] {processor.py:161} INFO - Started process (PID=240) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:38:49.807+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:38:49.811+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:38:49.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:38:49.828+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:38:49.878+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:38:49.878+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:38:49.900+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:38:49.900+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:38:49.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.118 seconds
[2024-09-06T07:39:20.006+0000] {processor.py:161} INFO - Started process (PID=247) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:20.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:39:20.009+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:20.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:20.028+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:20.057+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:20.057+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:39:20.078+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:20.078+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:39:20.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.096 seconds
[2024-09-06T07:39:50.481+0000] {processor.py:161} INFO - Started process (PID=254) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:50.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:39:50.486+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:50.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:50.502+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:50.499+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/newspipeline.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/newspipeline.py", line 190
    extract_task >> [upload_task >> load_task
                    ^
SyntaxError: '[' was never closed
[2024-09-06T07:39:50.515+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:50.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.108 seconds
[2024-09-06T07:39:52.639+0000] {processor.py:161} INFO - Started process (PID=261) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:52.640+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:39:52.642+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:52.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:53.282+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:55.967+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:55.967+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:39:55.985+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:55.985+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:39:56.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 3.465 seconds
[2024-09-06T07:39:57.154+0000] {processor.py:161} INFO - Started process (PID=262) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:57.156+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:39:57.159+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:57.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:57.191+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:39:57.204+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:57.203+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:39:57.231+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:39:57.230+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:39:57.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.115 seconds
[2024-09-06T07:40:07.208+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:07.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:40:07.211+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:07.211+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:07.237+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:07.250+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:07.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:40:07.279+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:07.279+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:40:07.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.100 seconds
[2024-09-06T07:40:14.367+0000] {processor.py:161} INFO - Started process (PID=264) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:14.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:40:14.370+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:14.369+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:14.392+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:14.402+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:14.402+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:40:14.424+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:14.424+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:40:14.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.083 seconds
[2024-09-06T07:40:44.490+0000] {processor.py:161} INFO - Started process (PID=272) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:44.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:40:44.493+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:44.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:44.511+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:40:44.646+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:44.645+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:40:44.664+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:40:44.664+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:40:44.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.225 seconds
[2024-09-06T07:41:14.690+0000] {processor.py:161} INFO - Started process (PID=279) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:14.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:14.692+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:14.692+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:14.808+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:14.841+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:14.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:14.864+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:14.864+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:14.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.204 seconds
[2024-09-06T07:41:16.861+0000] {processor.py:161} INFO - Started process (PID=280) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:16.862+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:16.864+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:16.863+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:16.887+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:16.915+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:16.915+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:16.935+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:16.935+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:16.958+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.103 seconds
[2024-09-06T07:41:18.002+0000] {processor.py:161} INFO - Started process (PID=281) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:18.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:18.005+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:18.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:18.029+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:18.055+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:18.054+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:18.074+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:18.074+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:18.094+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.097 seconds
[2024-09-06T07:41:20.028+0000] {processor.py:161} INFO - Started process (PID=282) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:20.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:20.030+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:20.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:20.052+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:20.079+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:20.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:20.100+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:20.100+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:20.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.100 seconds
[2024-09-06T07:41:21.167+0000] {processor.py:161} INFO - Started process (PID=283) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:21.168+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:21.170+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:21.169+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:21.196+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:21.226+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:21.225+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:21.250+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:21.250+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:21.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.119 seconds
[2024-09-06T07:41:23.020+0000] {processor.py:161} INFO - Started process (PID=290) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:23.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:23.022+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:23.022+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:23.045+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:23.752+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:23.751+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:23.958+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:23.958+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:23.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.966 seconds
[2024-09-06T07:41:54.137+0000] {processor.py:161} INFO - Started process (PID=297) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:54.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:54.140+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:54.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:54.168+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:54.204+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:54.203+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:54.224+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:54.224+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:54.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.112 seconds
[2024-09-06T07:41:55.149+0000] {processor.py:161} INFO - Started process (PID=298) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:55.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:55.151+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:55.151+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:55.174+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:55.329+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:55.329+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:55.349+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:55.348+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:55.371+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.228 seconds
[2024-09-06T07:41:57.300+0000] {processor.py:161} INFO - Started process (PID=299) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:57.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:57.303+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:57.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:57.326+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:57.339+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:57.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:57.363+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:57.363+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:57.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.094 seconds
[2024-09-06T07:41:59.431+0000] {processor.py:161} INFO - Started process (PID=300) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:59.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:41:59.433+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:59.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:59.453+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:41:59.463+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:59.463+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:41:59.483+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:41:59.483+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:41:59.502+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.076 seconds
[2024-09-06T07:42:27.565+0000] {processor.py:161} INFO - Started process (PID=307) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:42:27.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:42:27.568+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:42:27.567+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:42:27.595+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:42:27.742+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:42:27.741+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:42:27.766+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:42:27.766+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:42:27.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.231 seconds
[2024-09-06T07:42:57.977+0000] {processor.py:161} INFO - Started process (PID=315) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:42:57.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:42:57.980+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:42:57.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:42:58.000+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:42:58.031+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:42:58.031+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:42:58.053+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:42:58.053+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:42:58.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.101 seconds
[2024-09-06T07:44:27.829+0000] {processor.py:161} INFO - Started process (PID=316) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:44:27.839+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:44:27.848+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:44:27.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:44:36.297+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:44:59.100+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:44:59.093+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:45:00.282+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:45:00.282+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:45:00.327+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 32.847 seconds
[2024-09-06T07:53:15.340+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:53:15.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:53:15.639+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:53:15.639+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:53:15.972+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:53:16.132+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:53:16.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:53:16.151+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:53:16.151+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:53:16.172+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.837 seconds
[2024-09-06T07:53:46.409+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:53:46.432+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:53:46.448+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:53:46.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:53:46.604+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:53:46.736+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:53:46.736+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:53:46.854+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:53:46.854+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:53:46.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.544 seconds
[2024-09-06T07:54:17.391+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:54:17.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:54:17.394+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:54:17.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:54:17.413+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:54:17.446+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:54:17.445+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:54:17.469+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:54:17.469+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:54:17.495+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.111 seconds
[2024-09-06T07:54:47.578+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:54:47.579+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:54:47.580+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:54:47.580+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:54:47.598+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:54:47.636+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:54:47.636+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:54:47.666+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:54:47.665+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:54:47.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.113 seconds
[2024-09-06T07:55:17.796+0000] {processor.py:161} INFO - Started process (PID=83) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:55:17.797+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:55:17.798+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:55:17.798+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:55:17.815+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:55:17.847+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:55:17.847+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:55:17.879+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:55:17.878+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:55:17.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.107 seconds
[2024-09-06T07:55:48.020+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:55:48.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:55:48.023+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:55:48.022+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:55:48.039+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:55:48.067+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:55:48.066+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:55:48.087+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:55:48.087+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:55:48.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.092 seconds
[2024-09-06T07:56:18.131+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:56:18.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:56:18.134+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:56:18.134+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:56:18.149+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:56:18.175+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:56:18.175+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:56:18.196+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:56:18.196+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:56:18.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.087 seconds
[2024-09-06T07:56:48.337+0000] {processor.py:161} INFO - Started process (PID=104) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:56:48.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:56:48.339+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:56:48.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:56:48.355+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:56:48.385+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:56:48.385+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:56:48.407+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:56:48.407+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:56:48.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.109 seconds
[2024-09-06T07:57:18.549+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:57:18.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:57:18.552+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:57:18.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:57:18.567+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:57:18.597+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:57:18.597+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:57:18.617+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:57:18.617+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:57:18.633+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.089 seconds
[2024-09-06T07:57:48.671+0000] {processor.py:161} INFO - Started process (PID=118) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:57:48.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:57:48.674+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:57:48.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:57:48.692+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:57:48.730+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:57:48.729+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:57:48.754+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:57:48.754+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:57:48.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.113 seconds
[2024-09-06T07:58:18.896+0000] {processor.py:161} INFO - Started process (PID=125) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:58:18.897+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:58:18.899+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:58:18.898+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:58:18.914+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:58:18.940+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:58:18.940+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:58:18.963+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:58:18.962+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:58:18.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.088 seconds
[2024-09-06T07:58:49.017+0000] {processor.py:161} INFO - Started process (PID=132) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:58:49.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:58:49.020+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:58:49.019+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:58:49.037+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:58:49.072+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:58:49.071+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:58:49.096+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:58:49.095+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:58:49.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.103 seconds
[2024-09-06T07:59:19.207+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:59:19.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:59:19.209+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:59:19.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:59:19.227+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:59:19.256+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:59:19.256+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:59:19.279+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:59:19.279+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:59:19.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.097 seconds
[2024-09-06T07:59:49.536+0000] {processor.py:161} INFO - Started process (PID=146) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T07:59:49.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T07:59:49.561+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:59:49.560+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:59:49.609+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T07:59:49.685+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:59:49.685+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T07:59:49.707+0000] {logging_mixin.py:188} INFO - [2024-09-06T07:59:49.706+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T07:59:49.842+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.311 seconds
[2024-09-06T08:00:20.111+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:00:20.182+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:00:20.184+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:00:20.184+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:00:20.219+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:00:20.246+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:00:20.246+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:00:20.268+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:00:20.268+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:00:20.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.179 seconds
[2024-09-06T08:00:50.325+0000] {processor.py:161} INFO - Started process (PID=161) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:00:50.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:00:50.328+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:00:50.328+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:00:50.343+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:00:50.371+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:00:50.371+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:00:50.395+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:00:50.394+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:00:50.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.092 seconds
[2024-09-06T08:01:20.718+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:01:20.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:01:20.721+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:01:20.721+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:01:20.740+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:01:20.871+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:01:20.871+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:01:20.895+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:01:20.895+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:01:20.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.201 seconds
[2024-09-06T08:01:51.291+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:01:51.292+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:01:51.293+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:01:51.293+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:01:51.313+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:01:51.355+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:01:51.355+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:01:51.380+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:01:51.380+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:01:51.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.114 seconds
[2024-09-06T08:02:22.063+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:02:22.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:02:22.068+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:02:22.067+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:02:22.106+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:02:22.202+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:02:22.201+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:02:22.229+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:02:22.228+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:02:22.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.206 seconds
[2024-09-06T08:02:52.338+0000] {processor.py:161} INFO - Started process (PID=189) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:02:52.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:02:52.340+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:02:52.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:02:52.361+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:02:52.392+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:02:52.392+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:02:52.414+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:02:52.414+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:02:52.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.102 seconds
[2024-09-06T08:03:22.598+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:03:22.600+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:03:22.602+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:03:22.601+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:03:22.622+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:03:22.655+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:03:22.655+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:03:22.677+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:03:22.677+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:03:22.696+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.104 seconds
[2024-09-06T08:03:52.846+0000] {processor.py:161} INFO - Started process (PID=203) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:03:52.898+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:03:52.900+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:03:52.900+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:03:52.920+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:03:52.967+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:03:52.966+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:03:52.990+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:03:52.990+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:03:53.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.170 seconds
[2024-09-06T08:04:23.124+0000] {processor.py:161} INFO - Started process (PID=210) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:04:23.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:04:23.126+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:04:23.126+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:04:23.145+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:04:23.228+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:04:23.228+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:04:23.250+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:04:23.250+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:04:23.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.151 seconds
[2024-09-06T08:04:53.396+0000] {processor.py:161} INFO - Started process (PID=217) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:04:53.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:04:53.398+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:04:53.398+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:04:53.417+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:04:53.470+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:04:53.470+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:04:53.492+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:04:53.492+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:04:53.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.120 seconds
[2024-09-06T08:05:23.626+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:05:23.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:05:23.628+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:05:23.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:05:23.650+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:05:23.692+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:05:23.691+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:05:23.715+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:05:23.715+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:05:23.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.115 seconds
[2024-09-06T08:05:53.852+0000] {processor.py:161} INFO - Started process (PID=231) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:05:53.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:05:53.855+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:05:53.855+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:05:53.877+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:05:53.925+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:05:53.924+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:05:53.949+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:05:53.949+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:05:53.969+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.123 seconds
[2024-09-06T08:06:24.084+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:06:24.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:06:24.087+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:06:24.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:06:24.106+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:06:24.154+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:06:24.153+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:06:24.175+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:06:24.175+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:06:24.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.116 seconds
[2024-09-06T08:06:54.351+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:06:54.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:06:54.360+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:06:54.360+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:06:54.382+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:06:54.437+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:06:54.437+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:06:54.461+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:06:54.461+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:06:54.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.135 seconds
[2024-09-06T08:07:24.651+0000] {processor.py:161} INFO - Started process (PID=252) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:07:24.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:07:24.654+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:07:24.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:07:24.673+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:07:24.705+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:07:24.704+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:07:24.726+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:07:24.725+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:07:24.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.105 seconds
[2024-09-06T08:07:54.863+0000] {processor.py:161} INFO - Started process (PID=260) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:07:54.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:07:54.866+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:07:54.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:07:54.887+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:07:54.997+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:07:54.997+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:07:55.019+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:07:55.018+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:07:55.042+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.184 seconds
[2024-09-06T08:08:25.165+0000] {processor.py:161} INFO - Started process (PID=267) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:25.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:08:25.168+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:25.168+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:25.189+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:25.256+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:25.256+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:08:25.280+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:25.280+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:08:25.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.138 seconds
[2024-09-06T08:08:55.432+0000] {processor.py:161} INFO - Started process (PID=274) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:55.433+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:08:55.434+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:55.434+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:55.452+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:55.488+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:55.488+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:08:55.511+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:55.511+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:08:55.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.104 seconds
[2024-09-06T08:08:58.344+0000] {processor.py:161} INFO - Started process (PID=275) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:58.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:08:58.347+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:58.346+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:58.405+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:08:58.445+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:58.445+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:08:58.467+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:08:58.466+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:08:58.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.158 seconds
[2024-09-06T08:09:18.660+0000] {processor.py:161} INFO - Started process (PID=276) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:18.661+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:09:18.662+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:18.662+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:18.688+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:18.726+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:18.726+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:09:18.748+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:18.748+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:09:18.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.120 seconds
[2024-09-06T08:09:38.126+0000] {processor.py:161} INFO - Started process (PID=283) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:38.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:09:38.129+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:38.128+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:38.161+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:38.194+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:38.193+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:09:38.216+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:38.215+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:09:38.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.116 seconds
[2024-09-06T08:09:42.161+0000] {processor.py:161} INFO - Started process (PID=284) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:42.162+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:09:42.164+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:42.163+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:42.189+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:09:42.220+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:42.220+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:09:42.247+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:09:42.246+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:09:42.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.116 seconds
[2024-09-06T08:10:12.413+0000] {processor.py:161} INFO - Started process (PID=291) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:10:12.414+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:10:12.416+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:10:12.415+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:10:12.439+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:10:12.473+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:10:12.472+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:10:12.498+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:10:12.497+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:10:12.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.110 seconds
[2024-09-06T08:10:42.639+0000] {processor.py:161} INFO - Started process (PID=299) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:10:42.640+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:10:42.642+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:10:42.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:10:42.665+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:10:42.707+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:10:42.707+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:10:42.731+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:10:42.731+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:10:42.756+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.123 seconds
[2024-09-06T08:11:12.877+0000] {processor.py:161} INFO - Started process (PID=306) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:12.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:11:12.880+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:12.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:12.900+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:12.951+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:12.951+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:11:13.008+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:13.007+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:11:13.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.157 seconds
[2024-09-06T08:11:19.092+0000] {processor.py:161} INFO - Started process (PID=307) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:19.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:11:19.094+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:19.094+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:19.122+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:19.152+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:19.152+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:11:19.175+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:19.175+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:11:19.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.110 seconds
[2024-09-06T08:11:20.113+0000] {processor.py:161} INFO - Started process (PID=314) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:20.114+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:11:20.115+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:20.115+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:20.244+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:20.271+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:20.271+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:11:20.293+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:20.292+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:11:20.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.207 seconds
[2024-09-06T08:11:46.345+0000] {processor.py:161} INFO - Started process (PID=315) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:46.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:11:46.349+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:46.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:46.377+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:11:46.417+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:46.417+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:11:46.440+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:11:46.440+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:11:46.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.127 seconds
[2024-09-06T08:12:16.622+0000] {processor.py:161} INFO - Started process (PID=326) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:12:16.623+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:12:16.626+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:12:16.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:12:16.671+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:12:16.785+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:12:16.785+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:12:16.826+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:12:16.826+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:12:16.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.248 seconds
[2024-09-06T08:12:46.908+0000] {processor.py:161} INFO - Started process (PID=332) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:12:46.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:12:46.911+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:12:46.911+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:12:46.934+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:12:46.968+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:12:46.968+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:12:46.990+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:12:46.989+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:12:47.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.104 seconds
[2024-09-06T08:13:17.517+0000] {processor.py:161} INFO - Started process (PID=339) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:13:17.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:13:17.519+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:13:17.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:13:17.545+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:13:17.596+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:13:17.595+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:13:17.621+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:13:17.620+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:13:17.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.137 seconds
[2024-09-06T08:13:48.012+0000] {processor.py:161} INFO - Started process (PID=346) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:13:48.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:13:48.016+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:13:48.015+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:13:48.038+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:13:48.075+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:13:48.074+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:13:48.102+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:13:48.102+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:13:48.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.118 seconds
[2024-09-06T08:14:18.239+0000] {processor.py:161} INFO - Started process (PID=353) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:14:18.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:14:18.243+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:14:18.243+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:14:18.264+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:14:18.297+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:14:18.297+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:14:18.318+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:14:18.318+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:14:18.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.102 seconds
[2024-09-06T08:14:48.447+0000] {processor.py:161} INFO - Started process (PID=360) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:14:48.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:14:48.450+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:14:48.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:14:48.469+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:14:48.538+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:14:48.537+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:14:48.558+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:14:48.558+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:14:48.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.135 seconds
[2024-09-06T08:15:18.914+0000] {processor.py:161} INFO - Started process (PID=367) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:15:18.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:15:18.917+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:15:18.917+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:15:18.939+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:15:18.969+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:15:18.969+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:15:18.990+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:15:18.990+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:15:19.009+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.101 seconds
[2024-09-06T08:15:49.051+0000] {processor.py:161} INFO - Started process (PID=374) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:15:49.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:15:49.054+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:15:49.054+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:15:51.397+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:15:51.444+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:15:51.444+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:15:51.470+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:15:51.470+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:15:51.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 2.449 seconds
[2024-09-06T08:16:21.630+0000] {processor.py:161} INFO - Started process (PID=381) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:16:21.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:16:21.633+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:16:21.633+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:16:21.655+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:16:21.698+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:16:21.698+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:16:21.755+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:16:21.755+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:16:21.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.150 seconds
[2024-09-06T08:16:51.889+0000] {processor.py:161} INFO - Started process (PID=388) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:16:51.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:16:51.892+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:16:51.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:16:51.910+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:16:51.944+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:16:51.944+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:16:51.967+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:16:51.967+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:16:51.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.103 seconds
[2024-09-06T08:17:22.015+0000] {processor.py:161} INFO - Started process (PID=395) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:17:22.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:17:22.017+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:17:22.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:17:22.036+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:17:22.073+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:17:22.072+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:17:22.094+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:17:22.094+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:17:22.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.105 seconds
[2024-09-06T08:17:52.223+0000] {processor.py:161} INFO - Started process (PID=402) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:17:52.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:17:52.226+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:17:52.226+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:17:52.246+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:17:52.286+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:17:52.285+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:17:52.451+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:17:52.451+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:17:52.471+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.254 seconds
[2024-09-06T08:18:22.615+0000] {processor.py:161} INFO - Started process (PID=409) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:18:22.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:18:22.618+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:18:22.617+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:18:22.637+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:18:22.718+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:18:22.718+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:18:22.740+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:18:22.740+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:18:22.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.151 seconds
[2024-09-06T08:18:52.876+0000] {processor.py:161} INFO - Started process (PID=416) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:18:52.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:18:52.878+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:18:52.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:18:52.899+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:18:52.930+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:18:52.930+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:18:52.953+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:18:52.953+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:18:52.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.103 seconds
[2024-09-06T08:19:23.294+0000] {processor.py:161} INFO - Started process (PID=423) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:19:23.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:19:23.317+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:19:23.316+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:19:23.348+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:19:23.380+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:19:23.380+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:19:23.407+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:19:23.407+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:19:23.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.148 seconds
[2024-09-06T08:19:53.552+0000] {processor.py:161} INFO - Started process (PID=430) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:19:53.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:19:53.554+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:19:53.554+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:19:53.573+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:19:53.605+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:19:53.604+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:19:53.626+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:19:53.626+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:19:53.646+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.099 seconds
[2024-09-06T08:20:23.676+0000] {processor.py:161} INFO - Started process (PID=437) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:20:23.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:20:23.679+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:20:23.679+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:20:23.696+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:20:23.727+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:20:23.727+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:20:23.966+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:20:23.965+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:20:23.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.314 seconds
[2024-09-06T08:20:54.819+0000] {processor.py:161} INFO - Started process (PID=444) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:20:54.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:20:54.822+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:20:54.822+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:20:54.845+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:20:54.887+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:20:54.886+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:20:54.910+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:20:54.910+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:20:54.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.118 seconds
[2024-09-06T08:21:25.402+0000] {processor.py:161} INFO - Started process (PID=451) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:21:25.403+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:21:25.405+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:21:25.405+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:21:25.424+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:21:25.459+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:21:25.458+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:21:25.512+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:21:25.512+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:21:25.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.137 seconds
[2024-09-06T08:21:55.574+0000] {processor.py:161} INFO - Started process (PID=458) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:21:55.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:21:55.577+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:21:55.577+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:21:55.596+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:21:55.632+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:21:55.632+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:21:55.654+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:21:55.653+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:21:55.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.102 seconds
[2024-09-06T08:22:25.794+0000] {processor.py:161} INFO - Started process (PID=466) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:22:25.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:22:25.798+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:22:25.798+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:22:25.821+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:22:25.864+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:22:25.863+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:22:25.889+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:22:25.889+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:22:25.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.123 seconds
[2024-09-06T08:22:56.529+0000] {processor.py:161} INFO - Started process (PID=473) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:22:56.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:22:56.532+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:22:56.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:22:56.553+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:22:57.466+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:22:57.466+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:22:57.756+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:22:57.756+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:22:57.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 1.261 seconds
[2024-09-06T08:23:38.584+0000] {processor.py:161} INFO - Started process (PID=480) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:23:39.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:23:39.045+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:23:39.045+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:23:41.212+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:23:43.249+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:23:43.248+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:23:43.722+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:23:43.721+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:23:44.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 6.158 seconds
[2024-09-06T08:24:25.730+0000] {processor.py:161} INFO - Started process (PID=487) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:24:25.752+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:24:25.818+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:24:25.782+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:24:27.467+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:25:38.669+0000] {processor.py:161} INFO - Started process (PID=493) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:25:38.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:25:38.672+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:25:38.672+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:25:38.734+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:25:38.804+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:25:38.804+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:25:38.890+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:25:38.889+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:25:38.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.272 seconds
[2024-09-06T08:26:08.985+0000] {processor.py:161} INFO - Started process (PID=500) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:26:08.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:26:08.990+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:26:08.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:26:09.017+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:26:09.072+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:26:09.072+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:26:09.095+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:26:09.095+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:26:09.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.138 seconds
[2024-09-06T08:26:39.240+0000] {processor.py:161} INFO - Started process (PID=507) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:26:39.241+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:26:39.244+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:26:39.242+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:26:39.272+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:26:39.326+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:26:39.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:26:39.349+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:26:39.348+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:26:39.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.135 seconds
[2024-09-06T08:27:09.515+0000] {processor.py:161} INFO - Started process (PID=514) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:27:09.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:27:09.520+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:27:09.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:27:09.546+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:27:09.649+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:27:09.649+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:27:09.672+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:27:09.672+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:27:09.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.184 seconds
[2024-09-06T08:27:39.872+0000] {processor.py:161} INFO - Started process (PID=521) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:27:39.874+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:27:39.876+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:27:39.875+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:27:39.902+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:27:39.953+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:27:39.952+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:27:39.975+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:27:39.975+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:27:39.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.131 seconds
[2024-09-06T08:28:10.110+0000] {processor.py:161} INFO - Started process (PID=528) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:28:10.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:28:10.114+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:28:10.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:28:10.140+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:28:10.198+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:28:10.198+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:28:10.222+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:28:10.221+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:28:10.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.138 seconds
[2024-09-06T08:28:40.376+0000] {processor.py:161} INFO - Started process (PID=535) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:28:40.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:28:40.382+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:28:40.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:28:40.421+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:28:40.521+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:28:40.520+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:28:40.550+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:28:40.549+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:28:40.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.211 seconds
[2024-09-06T08:29:12.142+0000] {processor.py:161} INFO - Started process (PID=542) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:29:12.165+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:29:12.203+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:29:12.202+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:29:12.244+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:29:17.259+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:29:17.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:29:17.287+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:29:17.287+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:29:17.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 5.200 seconds
[2024-09-06T08:29:48.058+0000] {processor.py:161} INFO - Started process (PID=555) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:29:48.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:29:48.062+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:29:48.061+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:29:48.087+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:29:48.133+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:29:48.133+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:29:48.165+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:29:48.165+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:29:48.197+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.147 seconds
[2024-09-06T08:30:18.282+0000] {processor.py:161} INFO - Started process (PID=562) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:30:18.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:30:18.285+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:30:18.285+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:30:18.306+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:30:18.337+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:30:18.337+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:30:18.361+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:30:18.361+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:30:18.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.111 seconds
[2024-09-06T08:30:48.436+0000] {processor.py:161} INFO - Started process (PID=569) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:30:48.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:30:48.439+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:30:48.438+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:30:48.459+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:30:48.491+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:30:48.491+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:30:48.512+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:30:48.512+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:30:48.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.102 seconds
[2024-09-06T08:31:18.624+0000] {processor.py:161} INFO - Started process (PID=576) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:31:18.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:31:18.627+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:31:18.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:31:18.647+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:31:18.677+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:31:18.677+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:31:18.698+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:31:18.698+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:31:18.717+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.098 seconds
[2024-09-06T08:31:48.849+0000] {processor.py:161} INFO - Started process (PID=583) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:31:48.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:31:48.854+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:31:48.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:31:48.879+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:31:48.915+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:31:48.915+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:31:48.938+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:31:48.938+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:31:48.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.113 seconds
[2024-09-06T08:32:19.086+0000] {processor.py:161} INFO - Started process (PID=590) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:32:19.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:32:19.089+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:32:19.089+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:32:19.119+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:32:19.160+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:32:19.160+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:32:19.186+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:32:19.186+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:32:19.203+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.123 seconds
[2024-09-06T08:32:49.257+0000] {processor.py:161} INFO - Started process (PID=597) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:32:49.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:32:49.262+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:32:49.260+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:32:49.303+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:32:49.355+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:32:49.355+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:32:49.389+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:32:49.388+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:32:49.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.163 seconds
[2024-09-06T08:33:19.550+0000] {processor.py:161} INFO - Started process (PID=604) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:33:19.552+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:33:19.554+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:33:19.553+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:33:19.575+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:33:19.663+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:33:19.663+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:33:19.695+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:33:19.695+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:33:19.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.171 seconds
[2024-09-06T08:33:49.759+0000] {processor.py:161} INFO - Started process (PID=610) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:33:49.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:33:49.763+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:33:49.762+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:33:49.786+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:33:49.828+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:33:49.828+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:33:49.901+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:33:49.901+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:33:49.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.175 seconds
[2024-09-06T08:34:20.177+0000] {processor.py:161} INFO - Started process (PID=618) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:34:20.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:34:20.181+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:34:20.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:34:20.326+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:34:20.374+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:34:20.374+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:34:20.400+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:34:20.399+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:34:20.420+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.251 seconds
[2024-09-06T08:34:50.653+0000] {processor.py:161} INFO - Started process (PID=625) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:34:50.658+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:34:50.661+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:34:50.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:34:50.689+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:34:50.739+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:34:50.738+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:34:50.765+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:34:50.765+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:34:50.786+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.139 seconds
[2024-09-06T08:35:20.951+0000] {processor.py:161} INFO - Started process (PID=632) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:35:20.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:35:20.954+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:35:20.953+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:35:20.975+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:35:21.032+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:35:21.032+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:35:21.054+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:35:21.054+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:35:21.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.128 seconds
[2024-09-06T08:35:52.546+0000] {processor.py:161} INFO - Started process (PID=633) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:35:52.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:37:33.088+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:36:59.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:38:16.911+0000] {processor.py:161} INFO - Started process (PID=641) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:38:16.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:38:16.913+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:38:16.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:38:16.944+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:38:43.101+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:38:43.089+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:38:45.480+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:38:45.473+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:38:46.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 29.284 seconds
[2024-09-06T08:39:16.306+0000] {processor.py:161} INFO - Started process (PID=653) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:39:16.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:39:16.308+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:39:16.307+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:39:16.331+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:39:16.443+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:39:16.442+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:39:16.466+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:39:16.465+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:39:16.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.188 seconds
[2024-09-06T08:39:46.538+0000] {processor.py:161} INFO - Started process (PID=660) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:39:46.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:39:46.540+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:39:46.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:39:46.567+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:39:46.649+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:39:46.649+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:39:46.673+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:39:46.673+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:39:46.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.162 seconds
[2024-09-06T08:40:16.754+0000] {processor.py:161} INFO - Started process (PID=667) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:40:16.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:40:16.759+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:40:16.757+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:40:16.786+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:40:16.834+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:40:16.834+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:40:16.857+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:40:16.857+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:40:16.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.128 seconds
[2024-09-06T08:40:49.579+0000] {processor.py:161} INFO - Started process (PID=668) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:40:49.667+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:40:49.760+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:40:49.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:40:50.702+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:40:50.893+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:40:50.893+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:40:50.933+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:40:50.933+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:40:51.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 1.822 seconds
[2024-09-06T08:41:22.676+0000] {processor.py:161} INFO - Started process (PID=675) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:41:22.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:41:22.682+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:41:22.681+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:41:22.746+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:41:22.842+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:41:22.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:41:22.911+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:41:22.911+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:41:22.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.325 seconds
[2024-09-06T08:42:00.398+0000] {processor.py:161} INFO - Started process (PID=682) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:42:00.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:42:06.307+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:42:00.455+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:42:07.567+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:42:12.343+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:42:12.338+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:42:12.577+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:42:12.576+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:42:12.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 12.530 seconds
[2024-09-06T08:42:43.110+0000] {processor.py:161} INFO - Started process (PID=696) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:42:43.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:42:43.114+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:42:43.113+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:42:43.158+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:42:43.235+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:42:43.235+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:42:43.278+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:42:43.278+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:42:43.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.199 seconds
[2024-09-06T08:43:13.702+0000] {processor.py:161} INFO - Started process (PID=703) to work on /opt/airflow/dags/newspipeline.py
[2024-09-06T08:43:13.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/newspipeline.py for tasks to queue
[2024-09-06T08:43:13.706+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:43:13.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:43:13.737+0000] {processor.py:840} INFO - DAG(s) 'etl_data_pipeline' retrieved from /opt/airflow/dags/newspipeline.py
[2024-09-06T08:43:13.788+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:43:13.788+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-09-06T08:43:13.815+0000] {logging_mixin.py:188} INFO - [2024-09-06T08:43:13.814+0000] {dag.py:3954} INFO - Setting next_dagrun for etl_data_pipeline to 2024-09-06 00:00:00+00:00, run_after=2024-09-07 00:00:00+00:00
[2024-09-06T08:43:13.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/newspipeline.py took 0.143 seconds
